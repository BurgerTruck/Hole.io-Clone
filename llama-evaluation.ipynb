{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T00:56:12.671573Z","iopub.execute_input":"2025-01-23T00:56:12.671944Z","iopub.status.idle":"2025-01-23T00:59:35.862531Z","shell.execute_reply.started":"2025-01-23T00:56:12.671915Z","shell.execute_reply":"2025-01-23T00:59:35.861283Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\ndataset = load_dataset(\"BurgerTruck/counsel-chat\", split = \"test\")\nif True:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"BurgerTruck/LLAMA-Counsel-chat\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:01.855246Z","iopub.execute_input":"2025-01-23T01:00:01.855567Z","iopub.status.idle":"2025-01-23T01:00:37.925829Z","shell.execute_reply.started":"2025-01-23T01:00:01.855543Z","shell.execute_reply":"2025-01-23T01:00:37.925047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/2.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d4fc34da93943ca98755e923a6de4c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/333k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a4ce921c504c2785e5e5a6522bf0c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/350k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"872584c09efc46abb0e58ee59ff691e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2089 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0421f07c9f9b47d0870f7c0c4cfbd0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/261 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ff3e3982934f01b2e833f0f68f9350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/262 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16ee622a46343eab066b6fb1f18ec8d"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.48.1.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 6.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"088236d29ccc49278645317f7085c82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d818029cb94401bff4ac3ed019e6db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a5e8ebae6e41188adc575effc1c14b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"765eaf478e744e87979950a8b14cd22f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a58430945f054a3fba8dfebc99e5836d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291fc82f641c45a5b9f4e6f1c68e7098"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.1.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"speaker0\", \"assistant\" : \"speaker1\"} # for some reason does not work when chate template is llama\n\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:37.927382Z","iopub.execute_input":"2025-01-23T01:00:37.927608Z","iopub.status.idle":"2025-01-23T01:00:37.945253Z","shell.execute_reply.started":"2025-01-23T01:00:37.927588Z","shell.execute_reply":"2025-01-23T01:00:37.943767Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=14336, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 128006: <|start_header_id|>\n# 78191: assistant\n# 128007: <|end_header_id|>\n# 271: \\n\\n\n# 128009: <|eot_id|>\n\n# 128006: <|start_header_id|>\n# 882: user\n# 128007: <|end_header_id|>\nimport json\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    json_convs = [json.loads(convo) for convo in convos]\n    for conversation in json_convs:\n        for utterance_dict in conversation:\n            utterance_dict[\"role\"] = utterance_dict[\"from\"]\n            del utterance_dict[\"from\"]\n\n            utterance_dict[\"content\"] = utterance_dict[\"value\"]\n            del utterance_dict[\"value\"]\n\n            if utterance_dict[\"role\"] == \"speaker0\":\n                utterance_dict[\"role\"] = \"user\"\n            else:\n                utterance_dict[\"role\"] = \"assistant\"\n\n    for convo in json_convs:\n      yield tokenizer.apply_chat_template(convo, tokenize = True, add_generation_prompt = False, return_tensors='pt')\ndef create_assistant_mask(input_ids):\n    start_header_id = 128006\n    assistant_id = 78191\n    end_header_id = 128007\n    newline = 271\n    eos_token = 128009\n\n    # Squeeze the batch dimension if it exists\n    if len(input_ids.shape) == 2:\n        input_ids = input_ids.squeeze(0)\n\n    # Create an empty mask of the same size as input_ids initialized to 0\n    mask = torch.zeros_like(input_ids, dtype=torch.long)\n\n    # Iterate through the input_ids to detect the assistant's utterances\n    in_assistant_utterance = False\n    for i in range(3, len(input_ids)):\n        if (\n            input_ids[i - 3] == start_header_id\n            and input_ids[i - 2] == assistant_id\n            and input_ids[i - 1] == end_header_id\n            and input_ids[i] == newline\n            and not in_assistant_utterance\n        ):\n            # Found the start of an assistant's utterance\n            in_assistant_utterance = True\n        elif input_ids[i] == eos_token and in_assistant_utterance:\n            # End of assistant's utterance\n            in_assistant_utterance = False\n            mask[i] = 1\n        elif in_assistant_utterance:\n            mask[i] = 1\n\n    # Add batch dimension back if the input had one\n    if len(mask.shape) == 1:\n        mask = mask.unsqueeze(0)\n\n    return mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:37.946252Z","iopub.execute_input":"2025-01-23T01:00:37.946542Z","iopub.status.idle":"2025-01-23T01:00:42.663735Z","shell.execute_reply.started":"2025-01-23T01:00:37.946513Z","shell.execute_reply":"2025-01-23T01:00:42.662740Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(4000.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:42.664849Z","iopub.execute_input":"2025-01-23T01:00:42.665175Z","iopub.status.idle":"2025-01-23T01:00:42.680832Z","shell.execute_reply.started":"2025-01-23T01:00:42.665145Z","shell.execute_reply":"2025-01-23T01:00:42.680108Z"}},"outputs":[{"name":"stdout","text":"4000.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model.eval()\nimport torch.nn.functional as F\nwith torch.no_grad():\n  perplexity = 0\n  count = 0\n  for input_ids in formatting_prompts_func(dataset):\n    input_ids = input_ids.to(\"cuda\")\n    # for token in input_ids[0]:\n    #   print(f\"{token}: {tokenizer.decode(token)}\")\n    output = model(input_ids)\n    logits = output.logits\n\n    log_probs = F.log_softmax(logits, dim=-1)\n    target_ids = input_ids[:, 1:].clone() \n    padding = torch.full((input_ids.size(0), 1), 128009).to(\"cuda\")  # Padding token \n    target_ids = torch.cat([target_ids, padding], dim=1)  # Shift to the left by 1\n\n\n    # # Step 2: Gather log probabilities of the ground truth tokens\n    # # Use torch.gather to extract the probabilities corresponding to target tokens\n    target_log_probs = torch.gather(log_probs, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n    mask = create_assistant_mask(input_ids).to(\"cuda\")\n\n    # Check if user input is masked correctly \n    # for token in (input_ids*mask)[0]:\n    #   print(tokenizer.decode(token), end = '')\n\n    target_log_probs = target_log_probs * mask\n    # print(target_log_probs)\n    if torch.isnan(target_log_probs).any() or torch.isinf(target_log_probs).any():\n      print(\"Found nan or inf in target_log_probs!\")\n\n    # # Step 3: Calculate the average negative log-likelihood\n    nll = -target_log_probs.sum(dim=-1) / mask.sum(dim=-1)  # Mean over sequence length per batch\n    # # Step 4: Calculate perplexity for each sequence\n    perplexity +=nll[0]\n    count+=1\n    # if count==5:\n    #   break\nfinal_perplexity = torch.exp(perplexity/count)\nprint(perplexity)\nprint(final_perplexity)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:42.681649Z","iopub.execute_input":"2025-01-23T01:00:42.681953Z","execution_failed":"2025-01-23T01:03:11.866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}