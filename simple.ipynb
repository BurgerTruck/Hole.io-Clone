{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom unsloth.chat_templates import get_chat_template\nimport json\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"speaker0\", \"assistant\" : \"speaker1\"} # for some reason does not work when chate template is llama\n\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    json_convs = [json.loads(convo) for convo in convos]\n    for conversation in json_convs:\n        for utterance_dict in conversation:\n            utterance_dict[\"role\"] = utterance_dict[\"from\"]\n            del utterance_dict[\"from\"]\n\n            utterance_dict[\"content\"] = utterance_dict[\"value\"]\n            del utterance_dict[\"value\"]\n\n            if utterance_dict[\"role\"] == \"speaker0\":\n                utterance_dict[\"role\"] = \"user\"\n            else:\n                utterance_dict[\"role\"] = \"assistant\"\n            \n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in json_convs]\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(dataset_name, split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n\n# for convo in dataset['conversations']:\n#     try:\n#         texts = tokenizer.apply_chat_template(json.loads(convo), tokenize = False, add_generation_prompt = False) \n#     except:\n#         print(convo)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}