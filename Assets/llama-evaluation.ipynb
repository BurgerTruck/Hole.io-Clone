{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T00:56:12.671573Z","iopub.execute_input":"2025-01-23T00:56:12.671944Z","iopub.status.idle":"2025-01-23T00:59:35.862531Z","shell.execute_reply.started":"2025-01-23T00:56:12.671915Z","shell.execute_reply":"2025-01-23T00:59:35.861283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\ndataset = load_dataset(\"BurgerTruck/counsel-chat\", split = \"test\")\nif True:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"BurgerTruck/LLAMA-Counsel-chat\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:01.855246Z","iopub.execute_input":"2025-01-23T01:00:01.855567Z","iopub.status.idle":"2025-01-23T01:00:37.925829Z","shell.execute_reply.started":"2025-01-23T01:00:01.855543Z","shell.execute_reply":"2025-01-23T01:00:37.925047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"speaker0\", \"assistant\" : \"speaker1\"} # for some reason does not work when chate template is llama\n\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:37.927382Z","iopub.execute_input":"2025-01-23T01:00:37.927608Z","iopub.status.idle":"2025-01-23T01:00:37.945253Z","shell.execute_reply.started":"2025-01-23T01:00:37.927588Z","shell.execute_reply":"2025-01-23T01:00:37.943767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 128006: <|start_header_id|>\n# 78191: assistant\n# 128007: <|end_header_id|>\n# 271: \\n\\n\n# 128009: <|eot_id|>\n\n# 128006: <|start_header_id|>\n# 882: user\n# 128007: <|end_header_id|>\nimport json\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    json_convs = [json.loads(convo) for convo in convos]\n    for conversation in json_convs:\n        for utterance_dict in conversation:\n            utterance_dict[\"role\"] = utterance_dict[\"from\"]\n            del utterance_dict[\"from\"]\n\n            utterance_dict[\"content\"] = utterance_dict[\"value\"]\n            del utterance_dict[\"value\"]\n\n            if utterance_dict[\"role\"] == \"speaker0\":\n                utterance_dict[\"role\"] = \"user\"\n            else:\n                utterance_dict[\"role\"] = \"assistant\"\n\n    for convo in json_convs:\n      yield tokenizer.apply_chat_template(convo, tokenize = True, add_generation_prompt = False, return_tensors='pt')\ndef create_assistant_mask(input_ids):\n    start_header_id = 128006\n    assistant_id = 78191\n    end_header_id = 128007\n    newline = 271\n    eos_token = 128009\n\n    # Squeeze the batch dimension if it exists\n    if len(input_ids.shape) == 2:\n        input_ids = input_ids.squeeze(0)\n\n    # Create an empty mask of the same size as input_ids initialized to 0\n    mask = torch.zeros_like(input_ids, dtype=torch.long)\n\n    # Iterate through the input_ids to detect the assistant's utterances\n    in_assistant_utterance = False\n    for i in range(3, len(input_ids)):\n        if (\n            input_ids[i - 3] == start_header_id\n            and input_ids[i - 2] == assistant_id\n            and input_ids[i - 1] == end_header_id\n            and input_ids[i] == newline\n            and not in_assistant_utterance\n        ):\n            # Found the start of an assistant's utterance\n            in_assistant_utterance = True\n        elif input_ids[i] == eos_token and in_assistant_utterance:\n            # End of assistant's utterance\n            in_assistant_utterance = False\n            mask[i] = 1\n        elif in_assistant_utterance:\n            mask[i] = 1\n\n    # Add batch dimension back if the input had one\n    if len(mask.shape) == 1:\n        mask = mask.unsqueeze(0)\n\n    return mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:37.946252Z","iopub.execute_input":"2025-01-23T01:00:37.946542Z","iopub.status.idle":"2025-01-23T01:00:42.663735Z","shell.execute_reply.started":"2025-01-23T01:00:37.946513Z","shell.execute_reply":"2025-01-23T01:00:42.662740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(4000.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:42.664849Z","iopub.execute_input":"2025-01-23T01:00:42.665175Z","iopub.status.idle":"2025-01-23T01:00:42.680832Z","shell.execute_reply.started":"2025-01-23T01:00:42.665145Z","shell.execute_reply":"2025-01-23T01:00:42.680108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nimport torch.nn.functional as F\nwith torch.no_grad():\n  perplexity = 0\n  count = 0\n  for input_ids in formatting_prompts_func(dataset):\n    input_ids = input_ids.to(\"cuda\")\n    # for token in input_ids[0]:\n    #   print(f\"{token}: {tokenizer.decode(token)}\")\n    output = model(input_ids)\n    logits = output.logits\n\n    log_probs = F.log_softmax(logits, dim=-1)\n    target_ids = input_ids[:, 1:].clone() \n    padding = torch.full((input_ids.size(0), 1), 128009).to(\"cuda\")  # Padding token \n    target_ids = torch.cat([target_ids, padding], dim=1)  # Shift to the left by 1\n\n\n    # # Step 2: Gather log probabilities of the ground truth tokens\n    # # Use torch.gather to extract the probabilities corresponding to target tokens\n    target_log_probs = torch.gather(log_probs, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n    mask = create_assistant_mask(input_ids).to(\"cuda\")\n\n    # Check if user input is masked correctly \n    # for token in (input_ids*mask)[0]:\n    #   print(tokenizer.decode(token), end = '')\n\n    target_log_probs = target_log_probs * mask\n    # print(target_log_probs)\n    if torch.isnan(target_log_probs).any() or torch.isinf(target_log_probs).any():\n      print(\"Found nan or inf in target_log_probs!\")\n\n    # # Step 3: Calculate the average negative log-likelihood\n    nll = -target_log_probs.sum(dim=-1) / mask.sum(dim=-1)  # Mean over sequence length per batch\n    # # Step 4: Calculate perplexity for each sequence\n    perplexity +=nll[0]\n    count+=1\n    # if count==5:\n    #   break\nfinal_perplexity = torch.exp(perplexity/count)\nprint(perplexity)\nprint(final_perplexity)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T01:00:42.681649Z","iopub.execute_input":"2025-01-23T01:00:42.681953Z","execution_failed":"2025-01-23T01:03:11.866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}