{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y  \n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121 \n!pip install unsloth \n# pip install --upgrade typing-extensions ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nfrom unsloth.chat_templates import get_chat_template\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\n# del(model)\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model, \n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom transformers import TextStreamer\n\n# Persistent messages list\nmessages = []\n\n# Initialize TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\nwhile True:\n    # Add user input to the conversationH\n    user_input = input(\"You: \")\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    # Tokenize and prepare input for the model\n    chat_history = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(chat_history, return_tensors='pt').to(\"cuda\")\n    \n    # Generate response\n    output = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        streamer=None,  # Set to None for now; replace with `text_streamer` if desired\n        max_new_tokens=512,\n        pad_token_id=tokenizer.eos_token_id,\n        temperature = 0.8\n    )\n    \n    # Decode the entire output and slice to get the generated response\n    generated_tokens = output[0][inputs.input_ids.shape[1]:]  # Exclude input tokens\n    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n    print(f\"Assistant: {response}\")\n    \n    # Add assistant response to the conversation\n    messages.append({\"role\": \"assistant\", \"content\": response})","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}